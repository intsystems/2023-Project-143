\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newtheorem*{assumption*}{\assumptionnumber}
\providecommand{\assumptionnumber}{}
\makeatletter
\newenvironment{assumption}[2]
 {%
  \renewcommand{\assumptionnumber}{\textbf{Assumption} #1 ({#2})}%
  \begin{assumption*}%
  \protected@edef\@currentlabel{#1-#2}%
 }
 {%
  \end{assumption*}
 }


\title{Methods with preconditioning with weight decay regularization}

\author{ Kreinin M. \\
	Department of Intelligent Systems\\
	MIPT\\
	Moscow, Russia \\
	\texttt{kreinin.mv@phystech.edu} \\
	\And
	Beznosikov A. \\
	Department of Data Science\\
	MIPT\\
	Dolgoprudny, Russia \\
	\texttt{beznosikov.an@phystech.edu} \\
}
\date{}

\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Kreinin M.},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
	This paper examines the convergence rate of adaptive gradient methods when a regularization function is added to the target function. This is an area of research, since many machine learning problems use the heuristic, heuristic regularization, and we investigate the theoretical and practical convergence of adaptive gradient methods.
\end{abstract}

\keywords{Adam \and OASIS \and Regularization \and ADAHessian \and Weight Decay}

\section{Introduction}
In machine learning we consider unconstrained optimization problem
\begin{equation*}
	\min_{w \in \mathbb{R}^d} f(w)
\end{equation*}
Problems of the form (1) cover a plethora of applications, including empirical risk minimization,
deep learning, and supervised learning tasks such as regularized least squares or logistic regression. This minimization problem can be difficult to solve, particularly when the number of training
samples n, or problem dimension d, is large, or if the problem is nonconvex.

There are second order preconditioners methods for solving this problem, such as Adam \citep{kingma2014adam} , OASIS \citep{}, AdaHessian. 

Throughout this work we assume that each f $:\mathbb{R}^d \rightarrow \mathbb{R}$ is twice differentiable and also L-smooth. This is formalized in the following assumption.

\begin{assumption}{1}{Convex}
	The function f is convex, i.e. $\forall w, w' \in \mathbb{R}^d$
	\begin{equation}
		f(w) \geq f(w') + \langle \nabla F(w'), w-w' \rangle
	\end{equation}
\end{assumption}

\begin{assumption}{2}{L-smoothness}
	The gradients of F are L-Lipschitz continuous $\forall w \in \mathbb{R}^d$, i.e. there exists a constant $L > 0$ such that $\forall w, w' \in \mathbb{R}^d$,
	\begin{equation*}
		f(w) \leq f(w') + \langle \nabla f(w'), w-w' \rangle + \frac{L}{2} ||w - w'||^2
	\end{equation*}
\end{assumption}

\begin{assumption}{3}{Twice differentiable}
	The function f is twice continuously differentiable.
\end{assumption}

\begin{assumption}{4}{$\mu$ - strongly convex}
	The function f is $\mu$-strongly convex, i.e., there exists a constant $\mu > 0$ such that $\forall w, w' \in \mathbb{R}^d$
	\begin{equation*}
		f(w) \geq f(w') + \langle f(w'), w-w' \rangle +\frac{\mu}{2} ||w - w'||^2
	\end{equation*}
	
\end{assumption}


\section{Problem statement}
We consider the unconstrained optimization problem
\begin{equation*}
	\min_{w \in \mathbb{R}^d} f(w)
\end{equation*}
Problems of the form cover a plethora of applications, including empirical risk minimization,
deep learning, and supervised learning tasks such as regularized least squares or logistic regression.

But we can add regulirazation $r(w)$ -- regularization, and solve the unconstrained optimization problem.
\begin{equation*}
	\min_{w \in \mathbb{R}^d} F(w) = f(w) + r(w)
\end{equation*}

For solving these problems we always scaled adaptive gradients algorithms like ADAM, OASIS, ADAHessian. We investigate how the convergence rate will change when the regularization function is added to these algorithms.

% В исследуемом нами алгоритме сходимости мы будем исследовать алгоритмы двух следующих видов. В первом функция регуляризации выносится отдельном в пересчете весов модели, во втором функция домножается на обратный "гессиан".
In the convergence algorithm that we study, we will investigate algorithms of the following two kinds. In the first one, the regularization function is taken out separately in the recalculation of model weights, and in the second one, the function is dominated by the inverse "hessian".

\begin{algorithm}[H]
	\caption{Based algorithm}\label{alg:Based}
	$w_0, D_0, \eta_0$
	\begin{algorithmic}
		\For{$k = 1, 2, ...$}
		\State $D_k$ - update by using information of $f(w)$
		\State $\eta_k$ - update
		\State Set $w_{k+1} = w_k - \eta_k D_k^{-1} \nabla f(w_k) - \eta \nabla r(w)$ 
		\State or like that
		\State Set $w_{k+1} = w_k - \eta_k D_k^{-1} (\nabla f(w_k) + \nabla r(w))$
		\EndFor
	\end{algorithmic}
\end{algorithm}





\section{Experiment}
% Мы проведем эксперимент, в котором сравним Adam с добавленной L2 регуляризацией с AdamW, при различных learning rate и decoupled weight, также мы сравним его с измененным алгоритмом AdamW, где регуляризатор занесен под гессиан, также мы сравним его при условии, что теперь минимальное собственное число гессиана, это не epsilon, а гиперпараметр decoupled weight.Проведем те же эксперименты со SGD и L2 регуляризацией и SGD с decoupled weitght при тех же параметрах. Все эксперименты будут проводиться на батчах размера 128, на сетке ResNet18, в 200 эпох. Всё это будет реализовано с помощью библиотеки PyTorch на dataset CIFAR10.

We will conduct an experiment in which we compare Adam with added L2 regularization with AdamW, at different learning rate and decoupled weight, also we will compare it with the modified AdamW algorithm where the regularizer is put under the hessian, also we will compare it under the condition that now the minimal eigenvalue of hessian is not epsilon, but decoupled weight hyperparameter.We will conduct the same experiments with SGD and L2 regularization and SGD with decoupled weitght at the same parameters. All experiments will be performed on batches of size 128, on a ResNet18 grid of 200 epochs. All this will be implemented using the PyTorch library on dataset CIFAR10.


\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}