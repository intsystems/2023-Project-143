\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}

% Цвета 
\usepackage{xcolor}
\usepackage{color}
\usepackage{colortbl}

\newtheorem{theorem}{Theorem}

\newtheorem*{assumption*}{\assumptionnumber}
\providecommand{\assumptionnumber}{}
\makeatletter
\newenvironment{assumption}[2]
 {%
  \renewcommand{\assumptionnumber}{\textbf{Assumption} #1 ({#2})}%
  \begin{assumption*}%
  \protected@edef\@currentlabel{#1-#2}%
 }
 {%
  \end{assumption*}
 }


\title{Methods with preconditioning with weight decay regularization}

\author{ Kreinin M. \\
	Department of Intelligent Systems\\
	MIPT\\
	Moscow, Russia \\
	\texttt{kreinin.mv@phystech.edu} \\
	\And
	Beznosikov A. \\
	Department of Data Science\\
	MIPT\\
	Dolgoprudny, Russia \\
	\texttt{beznosikov.an@phystech.edu} \\
}
\date{}

\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Kreinin M.},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
	This paper examines the convergence rate of adaptive gradient methods when a regularization function is added to the target function. This is an area of research, since many machine learning problems use the heuristic, heuristic regularization, and we investigate the theoretical and practical convergence of adaptive gradient methods.
\end{abstract}

\keywords{Adam \and OASIS \and Regularization \and ADAHessian \and Weight Decay}

\section{Introduction}
In machine learning we consider unconstrained optimization problem
\begin{equation}
	\min_{w \in \mathbb{R}^d} f(w)
\end{equation}
Problems of the form (1) cover a plethora of applications, including empirical risk minimization,
deep learning \citep{lecun2015deep}, and supervised learning \citep{cunningham2008supervised} tasks such as regularized least squares \citep{rifkin2007notes} or logistic regression \citep{shalev2014understanding}.

The classic base method for solving the optimization is gradient descent, but this minimization problem can be difficult to solve, particularly when the number of training samples n, or problem dimension d, is large.
In modern machine learning especially large problems represent the greatest interest.
For such cases stochastic gradient descent \citep{bottou2010large} became popular solution.
Despite its simplicity, it proved itself to be an efficient and effective optimization method.
For a long time first-ordered methods were most popular approach of solving optimization problems.

Other way of solving the problem are methods with adaptive gradient \citep{wilson2017marginal}. These methods posses several superiority over first-ordered methods.
Firstly, they have bigger potential of distributed solving, because first ordered methods spend majority of time on "communication".
Secondly, they are less sensitive to the choice of hyperparameters up to the point that hyperparameters can be set equal to one.
Lastly, this methods often simply show faster convergence on modern large optimization problems, especially this methods became applicable in neural networks solving.
Nowadays it is known that preconditioning methods often outperform other methods on modern large optimization problems \citep{zhang2018three, yao2021adahessian, kingma2014adam, goldberg2011oasis}.

Preconditioning methods refer to techniques that involve scaling the gradient of a problem by a specific matrix $D_t$, which enables the gradient to take into account the geometry of the problem. In the classical case $D_t = (\nabla^2 f)^{-1}$, which corresponds newton's method, however hessian is difficult to calculate and even more difficult to reverse, because of that some heuristics are used to replace the reversed hessian \citep{dennis1977quasi}. In OASIS \citep{goldberg2011oasis} or AdaHessian \citep{yao2021adahessian} hessian is assumed to have diagonal dominance. In Adam \citep{kingma2014adam} gradient is simply normalized, etc. This heuristics were proved to be effective and efficient. General scheme of methods with preconditioning can be framed in the following algorithm

% записать в одну строчку, и пояснить g_t, D_t^{-1}
\begin{algorithm}[H]
            
            \caption{General scheme for preconditions methods}\label{alg:genalg}
    
            \begin{algorithmic}
            \small{
            \Require{$\eta, f$}
            
            \While {$w$ not converged}
            \State $t = t+1$
            \State $g_t \gets \nabla f(w_{t-1})$
            \State $D_t \gets$ pseudo-hessian, precondtioning, scaling
            \State $w_t = w_{t-1} - \eta \cdot g_t D_t^{-1}$    
            \EndWhile
            }
\end{algorithmic}
\end{algorithm}


Regularization is a powerful technique in machine learning that aims to prevent overfitting by adding additional constraints to the model.
It has been widely applied to various machine learning problems, including image classification, speech recognition, and natural language processing, and has shown its effectiveness in improving the generalization capability of neural networks \citep{poggio1987computational}.

In methods with preconditioning appears to be several ways to include regularization.
We can include regularizer $r$ in $g_t$ calculation so it will be taken into consideration while calculating $D_t$. This method is equal to considering optimization problem 

\begin{equation}
	\min_{w \in \mathbb{R}^d} f(w) + r(w).
\end{equation}

Or we can include regularizer only on last step, decreasing norm of $w$ \citep{loshchilov2017decoupled}.
This way of regularization is called weight decay and surprisingly turns out to be more efficient in practical problems.
There are few other ways of considering regularizer which will be discussed further in the paper.

In general, our paper provides insight into comparison of different consideration ways of regularization is methods with preconditioning. Here, we provide a brief summary of our main contributions:

\begin{itemize}
    \item \textbf{Proof of preconditioned methods' with weight decay convergence}.  We derive convergence guarantees for preconditioned methods considering assumptions of smoothness, strongly convex and PL-condition.
    
    \item \textbf{Research of the loss function} Comparison of accuracy and loss function for AdamW and AdamL2. As a result we saw that AdamW asymptotically converges to a non-zero value
    
    \item \textbf{Competitive Numerical Results} We investigate the empirical performance of Adam's variation including new one on a variety of standard machine learning tasks, including logistic regression.
\end{itemize}

\section{Problem statement}
We want to investigate the convergence speed of the AdamW method and the newly proposed MyAdamW method in machine learning problems, and we also plan to prove the convergence of these methods and investigate the obtained solution.

We consider the unconstrained optimization problem
\begin{equation*}
	\min_{w \in \mathbb{R}^d} f(w)
\end{equation*}

But we can add regulirazation $r(w)$ -- regularization, and solve the unconstrained optimization problem.
\begin{equation*}
	\min_{w \in \mathbb{R}^d} F(w) = f(w) + r(w)
\end{equation*}

In the convergence algorithm that we study, we will investigate algorithms of the following two kinds. In the first one, the regularization function is taken out separately in the recalculation of model weights, and in the second one, the function is dominated by the inverse "hessian".

\begin{algorithm}[H]
            
    \caption{General scheme for preconditions methods}\label{alg:genalg}

    \begin{algorithmic}
    \small{
    \Require{$\eta, \epsilon, f, r$}
    
    \While {$w$ not converged}
    \State $t = t+1$
    \State $g_t = \nabla f_t(w_{t-1})$
    \State $D_t =$ \textcolor{red}{diag($\sqrt{g_t \odot g_t} + \varepsilon$)}  \hfill \textcolor{red}{AdamW}
    \State $D_t$ = \textcolor{cyan}{$\mathbb{E}[z^T \nabla^2f(w_{t-1}) z]$} \hfill \textcolor{violet}{OASIS}, where $z$ in Rademacher distribution
    \State $w_t = w_{t-1} - \eta \cdot g_t D_t^{-1}$    
    \EndWhile
    }
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
            
    \caption{Adam($\lambda$)}\label{alg:Adam}

    \begin{algorithmic}
    \small{
    \Require{$\eta, \beta_1, \beta_2, \epsilon, f, r$}
    %\State $m_0 = 0$ -- 1-st moment vector
    %\State$v_0 = 0$ -- 2-nd moment vector
    \While {$\theta$ not converged}
    \State $t = t+1$
    \State $g_t = \nabla f(w_{t-1})$ + \textcolor{blue}{$\nabla r(w_{t-1})$} \hfill \textcolor{blue}{AdamL2}    
    \State $m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$
    \State $v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2$
    \State $\hat{m_t} = \frac{m_t}{1-\beta_1^t}$ + \textcolor{green}{$\nabla r(w_{t-1})$} \hfill \textcolor{green}{MyAdamW}
    \State $\hat{v_t} = \frac{v_t}{1-\beta_2^t}$ 
    \State $w_t = w_{t-1} - \eta \cdot \frac{\hat{m_t}}{\sqrt{v_t} + \epsilon}$    - \textcolor{red}{$\eta \nabla r(w_{t-1})$ } \hfill \textcolor{red}{AdamW}
    \EndWhile
    }
\end{algorithmic}
\end{algorithm}






\section{Experiment}
% Мы проведем эксперимент, в котором сравним Adam с добавленной L2 регуляризацией с AdamW, при различных learning rate и decoupled weight, также мы сравним его с измененным алгоритмом AdamW, где регуляризатор занесен под гессиан, также мы сравним его при условии, что теперь минимальное собственное число гессиана, это не epsilon, а гиперпараметр decoupled weight.Проведем те же эксперименты со SGD и L2 регуляризацией и SGD с decoupled weitght при тех же параметрах. Все эксперименты будут проводиться на батчах размера 128, на сетке ResNet18, в 200 эпох. Всё это будет реализовано с помощью библиотеки PyTorch на dataset CIFAR10.

We will conduct an experiment in which we compare Adam with added L2 regularization with AdamW, at different learning rate and decoupled weight, also we will compare it with the modified AdamW algorithm where the regularizer is put under the hessian, also we will compare it under the condition that now the minimal eigenvalue of hessian is not epsilon, but decoupled weight hyperparameter.We will conduct the same experiments with SGD and L2 regularization and SGD with decoupled weitght at the same parameters. All experiments will be performed on batches of size 128, on a ResNet18 grid of 200 epochs. All this will be implemented using the PyTorch library on dataset CIFAR10.

\section{Theory}
\begin{assumption}{1}{Convex}
	The function f is convex, i.e. $\forall w, w' \in \mathbb{R}^d$
	\begin{equation}
		f(w) \geq f(w') + \langle \nabla F(w'), w-w' \rangle
	\end{equation}
\end{assumption}

\begin{assumption}{2}{L-smoothness}
	The gradients of F are L-Lipschitz continuous $\forall w \in \mathbb{R}^d$, i.e. there exists a constant $L > 0$ such that $\forall w, w' \in \mathbb{R}^d$,
	\begin{equation*}
		f(w) \leq f(w') + \langle \nabla f(w'), w-w' \rangle + \frac{L}{2} ||w - w'||^2
	\end{equation*}
\end{assumption}

\begin{assumption}{3}{Twice differentiable}
	The function f is twice continuously differentiable.
\end{assumption}

\begin{assumption}{4}{$\mu$ - strongly convex}
	The function f is $\mu$-strongly convex, i.e., there exists a constant $\mu > 0$ such that $\forall w, w' \in \mathbb{R}^d$
	\begin{equation*}
		f(w) \geq f(w') + \langle f(w'), w-w' \rangle +\frac{\mu}{2} ||w - w'||^2
	\end{equation*}
\end{assumption}

To proof this algorigthm we use simplified form of AdamW algorithm
\begin{algorithm}[H]
\caption{AdamW}\label{alg:AdamW}
    
    \begin{algorithmic}
    \Require{$r, \varepsilon, f$}    
    \While {$w$ not converged}
        \State $t = t+1$
        \State $D_t = \text{diag}(|\nabla f(w_t)|_i)$
        \State $w_t = w_{t-1} - \eta \cdot \nabla f(w_t) D_t^{-1}  -\lambda \nabla r(w_t)$ 
    \EndWhile
\end{algorithmic}
\end{algorithm}

\begin{theorem}

Suppose the Assumption 1, 2, 5 and let $\varepsilon > 0$ and let the step-size satisfy
\begin{equation*}
    \eta < \frac{2 \alpha}{L + l \cdot \alpha} 
\end{equation*}
Then, the number of iterations performed by AdamW algorithm, starting from an initial point $w_0 \in \mathbb{R}^d$ with $\Delta_0 = \tilde{F}(w_0) - \tilde{F}^*$, required to obtain and $\varepsilon$-approximate solution of the convex problem (link here to problem 1) can be bounded by
\begin{equation*}
      T = \mathcal{O}\left( \frac{2\Delta_0 \Gamma \alpha } {(2\alpha - \tilde{L}\eta) \eta \varepsilon} \right)
\end{equation*}

\end{theorem}

\begin{theorem}
    Suppose the Assumption 1, 2, 4, 5 and let $\varepsilon > 0$ and let the step-size satisfy
    \begin{equation*}
        \eta \leq \frac{2 \alpha}{\tilde{L}}
    \end{equation*}
    Then, the number of iterations performed by AdamW algorithm, starting from an initial point $w_0 \in \mathbb{R}^d$ with $\Delta_0 = \tilde{F}(w_0) - \tilde{F}^*$, required to obtain and $\varepsilon$-approximate solution of the convex problem (link here to problem 1) can be bounded by
    \begin{equation*}
        T =  \mathcal{O}\left( \frac{\ln \frac{\Delta_0}{\epsilon}}{2 \mu \eta^2(\frac{1}{\eta} - \frac{\tilde{L}}{2 \alpha})} \right)
    \end{equation*}
\end{theorem}
\section{Appendix}

\begin{theorem}
Suppose the Assumption 1, 2, 5 and let $\varepsilon > 0$ and let the step-size satisfy
\begin{equation*}
    \eta < \frac{2 \alpha}{L + l \cdot \alpha} 
\end{equation*}
Then, the number of iterations performed by AdamW algorithm, starting from an initial point $w_0 \in \mathbb{R}^d$ with $\Delta_0 = \tilde{F}(w_0) - \tilde{F}^*$, required to obtain and $\varepsilon$-approximate solution of the convex problem (link here to problem 1) can be bounded by
\begin{equation*}
      T = \mathcal{O}\left( \frac{2\Delta_0 \Gamma \alpha } {(2\alpha - \tilde{L}\eta) \eta \varepsilon} \right)
\end{equation*}

\end{theorem}

\begin{proof}

Let's write first assumption for step t and $t+1$:

\begin{equation*}
    f(w_{t+1}) \leq f(w_t) + \langle \nabla f(w_t), w_{t+1} - w_t \rangle + \frac{L}{2}||w_{t+1} - w_t ||^2
\end{equation*}
Okay, by definition for our algorithm we have:

\begin{equation*}
w_{t+1} - w_t = -\eta D_t^{-1} \nabla f(w_t) - \eta \nabla r(w_t)    
\end{equation*}

and 

\begin{equation*}
\nabla f(w_t) = \frac{1}{\eta} D^t(w_t - w_{t+1}) - D^t \nabla r(w_t)
\end{equation*}

Okay, now let's replace $\nabla f(w_t)$ and $I \leq \frac{D_t}{\alpha}$
\begin{equation*}
    f(w_{t+1}) \leq f(w_t) + \langle \frac{1}{\eta}D_t(w_t - w_{t+1}) - D_t\nabla r(w_t), w_{t+1} - w_t \rangle + \frac{L}{2 \alpha} ||w_{t+1} - w_t||_{D_t}^2
\end{equation*}

\begin{equation*}
    f(w_{t+1}) \leq f(w_t) + \left(\frac{L}{2 \alpha} - \frac{1}{\eta} \right) ||w_{t+1} - w_t||_{D_t}^2 - \langle D_t \nabla r(w_t), w_{t+1} - w_t \rangle
\end{equation*}

Lets define new variable $\tilde{r} : \nabla \tilde{r} = D_t \nabla r(w_t)$. Then rewrite step using the variable and 5-th assumption.
\begin{equation*}
    \tilde{r}(w_{t+1}) \leq \tilde{r}(w_t) + \langle \tilde{r}(w_t), w_{t+1} - w_t \rangle + \frac{l}{2} (w_{t+1} - w_t)^T D_t (w_{t+1} - w_t)
\end{equation*}

\begin{equation*}
    f(w_{t+1}) \leq f(w_t) + \left( \frac{L}{2\alpha} - \frac{1}{\eta} \right) ||w_{t+1} - w_t||_{D_t}^2 + \tilde{r}(w_t) - \tilde{r}(w_{t+1}) + \frac{l}{2}||w_{t+1}-w_t||_{D_t}^2
\end{equation*}

$\tilde{F}(w) = f(w) + \tilde{r}(w)$, $F(w) = f(w) + r(w)$, ($\tilde{L}=L + l \alpha$), we get:

\begin{equation*}
    \tilde{F}(w_{t+1}) \leq \tilde{F}(w_t) + \left( \frac{\tilde{L}}{2\alpha} - \frac{1}{\eta}  \right) ||w_{t+1} - w_t||_{D_t}^2
\end{equation*}


\begin{equation*}
    \left(\frac{1}{\eta} - \frac{\tilde{L}}{2\alpha}   \right) ||w_{t+1} - w_t||_{D_t}^2 \leq \tilde{F}(w_t) - \tilde{F}(w_{t+1})
\end{equation*}

\begin{equation*}
    \frac{\eta^2  (T+1)}{\Gamma}\left(\frac{1}{\eta} - \frac{\tilde{L}}{2\alpha}   \right)\cdot\min_{k = 0, T} ||\nabla f(w_t) + \nabla \tilde{r}(w_t)||^2 \leq \frac{\eta^2}{\Gamma}\left(\frac{1}{\eta} - \frac{\tilde{L}}{2\alpha}   \right)\cdot\sum\limits_{t = 0}^T ||\nabla f(w_t) + \nabla \tilde{r}(w_t)||^2 \leq \tilde{F}(w_0) - \tilde{F}(w_*)
\end{equation*}


\begin{equation*}
    \min_{t = 0, T} ||\nabla f(w_t) + \nabla \tilde{r}(w_t)||^2 \leq \frac{(\tilde{F}(w_0) - \tilde{F}(w_*))\Gamma}{(\frac{1}{\eta} - \frac{\tilde{L}}{2\alpha}) \eta^2 (T+1)} = \varepsilon
\end{equation*}

\begin{equation*}
    T + 1 \geq \frac{\Delta_0 \Gamma}{(\frac{1}{\eta} - \frac{\tilde{L}}{2\alpha}) \eta^2 \varepsilon}
\end{equation*}

Then:
\begin{equation*}
      T = \mathcal{O}\left( \frac{2\Delta_0 \Gamma \alpha } {(2\alpha - \tilde{L}\eta) \eta \varepsilon} \right)
\end{equation*}
\end{proof}

\begin{theorem}
    Suppose the Assumption 1, 2, 4, 5 and let $\varepsilon > 0$ and let the step-size satisfy
    \begin{equation*}
        \eta \leq \frac{2 \alpha}{\tilde{L}}
    \end{equation*}
    Then, the number of iterations performed by AdamW algorithm, starting from an initial point $w_0 \in \mathbb{R}^d$ with $\Delta_0 = \tilde{F}(w_0) - \tilde{F}^*$, required to obtain and $\varepsilon$-approximate solution of the convex problem (link here to problem 1) can be bounded by
    \begin{equation*}
        T =  \mathcal{O}\left( \frac{\ln \frac{\Delta_0}{\epsilon}}{2 \mu \eta^2(\frac{1}{\eta} - \frac{\tilde{L}}{2 \alpha})} \right)
    \end{equation*}
\end{theorem}

\begin{proof}
    Assume 
    \begin{equation}
    \nabla \tilde{F} = \nabla f + \nabla \tilde{r}    
    \end{equation}
    
    \begin{equation}
    L + ||D_t||l = \tilde{L}        
    \end{equation}
    \begin{equation*}
    w_{t+1} - w_t = -\eta D_t^{-1} \nabla r(w_t) - \eta \nabla r(w_t) = -\eta D_t^{-1} (\nabla f + \nabla \tilde{r})(w_t) = -\eta D_t^{-1} \nabla \tilde{F}(w_t)    
    \end{equation*}
    Then we write $\tilde{L}$-smoothness for $\tilde{F}$ 
    \begin{equation*}
        \tilde{F}(w_{t+1}) - \tilde{F}(w_t) \leq  \langle \nabla \tilde{F}(w_t), w_{t+1} - w_t \rangle + \frac{\tilde{L}}{2} ||w_{t+1} - w_t||^2
    \end{equation*}
    \begin{equation*}
        \tilde{F}(w_{t+1}) - \tilde{F}(w_t) \leq - \langle \frac{1}{\eta} D_t(w_{t+1} - w_t), w_{t+1} - w_t \rangle + \frac{\tilde{L}}{2} ||w_{t+1} - w_t||^2 = (\frac{\tilde{L}}{2 \alpha} - \frac{1}{\eta}) ||w_{t+1} - w_t||^2_{D_t} 
    \end{equation*}
    \begin{equation*}
        = (\frac{\tilde{L}}{2 \alpha} - \frac{1}{\eta}) ||w_{t+1} - w_t||^2_{D_t} = (\frac{\tilde{L}}{2 \alpha} - \frac{1}{\eta}) ||-\eta D_t^{-1} \nabla \tilde{F}(w_t)||_{D_t}^2  \leq (\frac{\tilde{L}}{2 \alpha} - \frac{1}{\eta}) \eta^2 ||\nabla \tilde{F}(w_t)||^2_{D_t^{-1}}
    \end{equation*}
    Then we use PL-condition for the function $\tilde{F}$:
    \begin{equation*}
        ||\nabla \tilde{F}(w_t)||_{D_t^{-1}}^2 \geq 2 \mu (\tilde{F}(w_t) - \tilde{F}^*)
    \end{equation*}

    \begin{equation*}
    \tilde{F}(w_{t}) -  F^* \ge \tilde{F}(w_{t+1}) - \tilde{F}^* + (\frac{1}{\eta} - \frac{\tilde{L}}{2 \alpha}) \eta^2 2 \mu (\tilde{F}(w_t) - \tilde{F}^*) = \left( 1 +  2 \mu \eta^2(\frac{1}{\eta} - \frac{\tilde{L}}{2 \alpha}) \right) (\tilde{F}(w_{t+1}) - \tilde{F}^*)        
    \end{equation*}


    \begin{equation*}
    \epsilon \ge \Delta_0 \left( 1 +  2 \mu \eta^2(\frac{1}{\eta} - \frac{\tilde{L}}{2 \alpha}) \right)^{-T} \ge (\tilde{F}(w_{T}) - \tilde{F}^*)        
    \end{equation*}

    \begin{equation*}
    T = \frac{\ln \frac{\Delta_0}{\epsilon}}{\ln(1 + 2 \mu \eta^2(\frac{1}{\eta} - \frac{\tilde{L}}{2 \alpha}))} \approx \frac{\ln \frac{\Delta_0}{\epsilon}}{2 \mu \eta^2(\frac{1}{\eta} - \frac{\tilde{L}}{2 \alpha})}        
    \end{equation*}
    Then:
    \begin{equation*}
    T =  \mathcal{O}\left( \frac{\ln \frac{\Delta_0}{\epsilon}}{2 \mu \eta^2(\frac{1}{\eta} - \frac{\tilde{L}}{2 \alpha})} \right)
    \end{equation*}
\end{proof}


\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}