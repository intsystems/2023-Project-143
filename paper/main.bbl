\begin{thebibliography}{18}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Chapelle et~al.(2000)Chapelle, Weston, Bottou, and
  Vapnik]{chapelle2000vicinal}
Olivier Chapelle, Jason Weston, L{\'e}on Bottou, and Vladimir Vapnik.
\newblock Vicinal risk minimization.
\newblock \emph{Advances in neural information processing systems}, 13, 2000.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock \emph{nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Cunningham et~al.(2008)Cunningham, Cord, and
  Delany]{cunningham2008supervised}
P{\'a}draig Cunningham, Matthieu Cord, and Sarah~Jane Delany.
\newblock Supervised learning.
\newblock \emph{Machine learning techniques for multimedia: case studies on
  organization and retrieval}, pages 21--49, 2008.

\bibitem[Rifkin and Lippert(2007)]{rifkin2007notes}
Ryan~M Rifkin and Ross~A Lippert.
\newblock Notes on regularized least squares.
\newblock 2007.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{shalev2014understanding}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem[Hazan et~al.(2007)Hazan, Rakhlin, and Bartlett]{hazan2007adaptive}
Elad Hazan, Alexander Rakhlin, and Peter Bartlett.
\newblock Adaptive online gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems}, 20, 2007.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Goldberg et~al.(2011)Goldberg, Zhu, Furger, and Xu]{goldberg2011oasis}
Andrew Goldberg, Xiaojin Zhu, Alex Furger, and Jun-Ming Xu.
\newblock Oasis: Online active semi-supervised learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~25, pages 362--367, 2011.

\bibitem[Zhang et~al.(2018)Zhang, Wang, Xu, and Grosse]{zhang2018three}
Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse.
\newblock Three mechanisms of weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1810.12281}, 2018.

\bibitem[Yao et~al.(2021)Yao, Gholami, Shen, Mustafa, Keutzer, and
  Mahoney]{yao2021adahessian}
Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and
  Michael Mahoney.
\newblock Adahessian: An adaptive second order optimizer for machine learning.
\newblock In \emph{proceedings of the AAAI conference on artificial
  intelligence}, volume~35, pages 10665--10673, 2021.

\bibitem[Dennis and Mor{\'e}(1977)]{dennis1977quasi}
John~E Dennis, Jr and Jorge~J Mor{\'e}.
\newblock Quasi-newton methods, motivation and theory.
\newblock \emph{SIAM review}, 19\penalty0 (1):\penalty0 46--89, 1977.

\bibitem[Jahani et~al.(2021)Jahani, Rusakov, Shi, Richt{\'a}rik, Mahoney, and
  Tak{\'a}{\v{c}}]{jahani2021doubly}
Majid Jahani, Sergey Rusakov, Zheng Shi, Peter Richt{\'a}rik, Michael~W
  Mahoney, and Martin Tak{\'a}{\v{c}}.
\newblock Doubly adaptive scaled algorithm for machine learning using
  second-order information.
\newblock \emph{arXiv preprint arXiv:2109.05198}, 2021.

\bibitem[Zhu et~al.(2017)Zhu, Li, Ouyang, Yu, and Wang]{zhu2017learning}
Feng Zhu, Hongsheng Li, Wanli Ouyang, Nenghai Yu, and Xiaogang Wang.
\newblock Learning spatial regularization with image-level supervisions for
  multi-label image classification.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 5513--5522, 2017.

\bibitem[Zhou et~al.(2017)Zhou, Xiong, and Socher]{zhou2017improved}
Yingbo Zhou, Caiming Xiong, and Richard Socher.
\newblock Improved regularization techniques for end-to-end speech recognition.
\newblock \emph{arXiv preprint arXiv:1712.07108}, 2017.

\bibitem[Wu et~al.(2022)Wu, Ding, Tang, Zhang, Qin, and Liu]{wu2022stgn}
Tingting Wu, Xiao Ding, Minji Tang, Hao Zhang, Bing Qin, and Ting Liu.
\newblock Stgn: an implicit regularization method for learning with noisy
  labels in natural language processing.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 7587--7598, 2022.

\bibitem[Girosi et~al.(1995)Girosi, Jones, and
  Poggio]{girosi1995regularization}
Federico Girosi, Michael Jones, and Tomaso Poggio.
\newblock Regularization theory and neural networks architectures.
\newblock \emph{Neural computation}, 7\penalty0 (2):\penalty0 219--269, 1995.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\end{thebibliography}
