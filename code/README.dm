There a several experiments that made in different files.

1. Logregression.ipynb были сделано несколько экспериментов на следующих датасетах: 'mushrooms', 'wine', 'water pobability', 'diabetes'. 
Указанные выше датасеты находятся в данной же директории, класс логрегрессии и методы оптимизации такие, как AdamW, AdamWH, ADAML2, GD, OASIS, OASISW, OASISL2, OASISWH
находятся в файле lib.py, в котором реализован класс MyLogregression, которые поддерживает все выше указанные методы, а также замеряют критерий и ошибку на тестовой выборке. 
Для проведения экспериментов нужно последовательно запустить все клетки из файла Logregression.ipynb.

2. Следующий эксперимент это сравнение методов оптимизации при обучении нейронных сетей, для этого нужно запустить файл main.py, через командную строку, если вы из под windows:
python main.py. В файле resnet.py реализована нейронная сеть ResNet18, на которой собственно и сравниваются методы оптимизации. ResNet18 обучается на CIFAR10, он автоматически скачается в ту же директорию, в которой
и находится файл main.py.

3. И оставшийся третий эксперимент это сравнение критерий остановки, в нашем случае это норма градиента целевой функции и норма градиента настоящей целевой функции AdamW, в файлах
s_adamw.py, s_adamwh.py, s_sgd.py, реализованы следующие методы оптимизации: AdamW, AdamWH, SGD, в которых также записывается критерий на каждом из шагов. Для выполнения эксперимента нужна библиотека pytorch, и последовательно запустить все клетки в файле main.py 
из директории pytorch logreg.
