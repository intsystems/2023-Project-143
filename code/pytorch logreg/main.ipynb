{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# loading training data\n",
    "train_dataset = datasets.MNIST(root='./data', \n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "#loading test data\n",
    "test_dataset = datasets.MNIST(root='./data', \n",
    "                              train=False, \n",
    "                              transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):    \n",
    "    # build the constructor\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(n_inputs, n_outputs)\n",
    "    # make predictions\n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    " \n",
    "# load train and test data samples into dataloader\n",
    "batach_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batach_size, shuffle=True) \n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batach_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(optimizer, device, log_regr):\n",
    "    # defining Cross-Entropy loss\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    epochs = 50\n",
    "    loss_grad_regular = []\n",
    "    loss_grad_irregular = []\n",
    "    Loss = []\n",
    "    acc = []\n",
    "    inputs = 28*28\n",
    "    for epoch in range(epochs):\n",
    "        avg_grad_regular = 0\n",
    "        avg_grad_irregular = 0\n",
    "        t = 0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            t += 1\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = log_regr(images.view(-1, inputs))\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_grad_regular += optimizer._grad_norm[0]\n",
    "            avg_grad_irregular += optimizer._grad_norm[1]\n",
    "\n",
    "        loss_grad_regular.append(avg_grad_regular/t)\n",
    "        loss_grad_irregular.append(avg_grad_irregular/t)\n",
    "        \n",
    "        Loss.append(loss.item())\n",
    "        \n",
    "        correct = 0\n",
    "        \n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = log_regr(images.view(-1, inputs))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum()\n",
    "\n",
    "        accuracy = 100 * (correct.item()) / len(test_dataset)\n",
    "        acc.append(accuracy)\n",
    "        print('Epoch: {}. Loss: {}. Accuracy: {}. Grad: {}. New_Grad: {}.'.format(epoch, loss.item(), accuracy, avg_grad_regular, avg_grad_irregular))\n",
    "    return Loss, acc, loss_grad_regular, loss_grad_irregular\n",
    "\n",
    "def train_models(learning_rates=[0.1 , 0.01 , 0.001 ], \n",
    "                 weight_decayes=[0.01, 0.001, 0.0001], name=\"AdamL2\", optim=torch.optim.Adam):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    models = []\n",
    "    names = []\n",
    "    Loss = []\n",
    "    Acc = []\n",
    "    Loss_grad_regular = []\n",
    "    Loss_grad_irregular = []\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        for wd in weight_decayes:\n",
    "            # instantiate the model\n",
    "            n_inputs = 28*28 # makes a 1D vector of 1024\n",
    "            n_outputs = 10\n",
    "            log_regr = LogisticRegression(n_inputs, n_outputs)\n",
    "            log_regr = log_regr.to(device)\n",
    "            if device == 'cuda':\n",
    "                log_regr = torch.nn.DataParallel(log_regr)\n",
    "                cudnn.benchmark = True\n",
    "                    \n",
    "            optimizer = optim(log_regr.parameters(),lr=lr, weight_decay=wd)\n",
    "            loss, acc, loss_grad_regular, loss_grad_irregular = training(optimizer, device=device, log_regr=log_regr)\n",
    "            \n",
    "            Loss.append(loss)\n",
    "            Acc.append(acc)\n",
    "            Loss_grad_regular.append(loss_grad_regular)\n",
    "            Loss_grad_irregular.append(loss_grad_irregular)\n",
    "            names.append(name+f\"(lr={lr}, wd={wd})\")\n",
    "            models.append(optimizer)\n",
    "\n",
    "    return models, names, Loss, Acc, Loss_grad_regular, Loss_grad_irregular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import s_adamw\n",
    "reload(s_adamw)\n",
    "from s_adamw import our_AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# instantiate the model\n",
    "n_inputs = 28*28 # makes a 1D vector of 784\n",
    "n_outputs = 10\n",
    "log_regr = LogisticRegression(n_inputs, n_outputs)\n",
    "log_regr = log_regr.to(device)\n",
    "if device == 'cuda':\n",
    "    log_regr = torch.nn.DataParallel(log_regr)\n",
    "    cudnn.benchmark = True\n",
    "Loss, acc, loss_grad_regular, loss_grad_irregular = training(our_AdamW(log_regr.parameters(),lr=0.1, weight_decay=0.01, betas=(0.9, 0.999)), device, log_regr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part for kaggle\n",
    "# import module we'll need to import our custom module\n",
    "#from shutil import copyfile\n",
    "\n",
    "# copy our file into the working directory (make sure it has .py suffix)\n",
    "#copyfile(src = \"../input/optimizers/adamwh.py\", dst = \"../working/adamwh.py\")\n",
    "\n",
    "# import all our functions\n",
    "#from importlib import reload\n",
    "#import adamwh\n",
    "#reload(adamwh)\n",
    "#from adamwh import AdamWH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
